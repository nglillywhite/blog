{
 "cells": [
  {
   "cell_type": "raw",
   "id": "028b47ea-a86e-4958-90b5-4368bfcbc1b9",
   "metadata": {},
   "source": [
    "---\n",
    "badges: true\n",
    "categories:\n",
    "- jupyter\n",
    "- fastai\n",
    "- ML\n",
    "date: '2023-01-08'\n",
    "description: Transformer Models & Huggingface Libraries\n",
    "title: Fastai Chapter 4 | Natural Language Processing\n",
    "author: Nicholas Lillywhite\n",
    "draft: true\n",
    "toc: true\n",
    "image: \"images/sgd.png\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57e1588-271b-42e0-8bee-517fa43bbd28",
   "metadata": {},
   "source": [
    "# Chapter 4 | Natural Language Processing, Transformers, Huggingface\n",
    "> A New Library, New Model Architectures, And Natural Language Data\n",
    ">Checkout this notebook in [colab](https://colab.research.google.com/github/nglillywhite/blog/blob/main/posts/wotwot)\n",
    "\n",
    "This week focuses on building models with interact with natural language which is very different from images or structured tabular data. Jeremy starts off discussing some model architectures (some of which he pioneered with collaborators) like 'ULMFit', 'Transformers', and Recurrent Neural Nets (RNNs) which are often seen in the news headlines as being significant. This week will also use huggingface, which is another library from fastai but is seemingly the best library as of writing this for working with language. We're going to dive into tokenising and other natural language specific problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba12f15a-73f3-4fce-8895-107e97695a7d",
   "metadata": {},
   "source": [
    "## Lecture Content\n",
    "### US Patent Phrase Matching\n",
    "\n",
    "We're working with the [US Patent Phrase Matching](https://www.kaggle.com/competitions/us-patent-phrase-to-phrase-matching) Kaggle Competition data for this lesson lecture, I've just visited the page and downloaded the dataset into my local data path but you can of course use the kaggle APIs.\n",
    "\n",
    "This competition in particular is about comparing two short phrases and whether they are similar to each other based on which patent class they were used with. Scores of 1 being identical and 0 meaning they are different. The scores are represented in a set of 0, 0.25, 0.5, and 1 which acts like a classification problem rather than regression because they're distinct instead of smooth.\n",
    "\n",
    "Jeremy starts by proposing that we can feed this into our model successfully by representing the data as \"TEXT1: abatement; TEXT2: eliminating process\" of which we then pick a distinct category from the above. To my understand this relates to the 'anchors' and 'target' in this dataset which we'll look at below.\n",
    "\n",
    "### EDA of Patent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02d1803b-d1a6-4800-8f43-d76dd8e78503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastai.vision.all import *\n",
    "\n",
    "data_path = Path(\"../data/us-patent-phrase-to-matching/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45a95a38-5df9-4f1c-83a9-770106d75eec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('../data/us-patent-phrase-to-matching/test.csv'),Path('../data/us-patent-phrase-to-matching/train.csv')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8b2f8d6-9874-48f9-a3cc-ac6986856445",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>abatement</td>\n",
       "      <td>abatement of pollution</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7b9652b17b68b7a4</td>\n",
       "      <td>abatement</td>\n",
       "      <td>act of abating</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>36d72442aefd8232</td>\n",
       "      <td>abatement</td>\n",
       "      <td>active catalyst</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5296b0c19e1ce60e</td>\n",
       "      <td>abatement</td>\n",
       "      <td>eliminating process</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>54c1e3b9184cb5b6</td>\n",
       "      <td>abatement</td>\n",
       "      <td>forest region</td>\n",
       "      <td>A47</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id     anchor                  target context  score\n",
       "0  37d61fd2272659b1  abatement  abatement of pollution     A47   0.50\n",
       "1  7b9652b17b68b7a4  abatement          act of abating     A47   0.75\n",
       "2  36d72442aefd8232  abatement         active catalyst     A47   0.25\n",
       "3  5296b0c19e1ce60e  abatement     eliminating process     A47   0.50\n",
       "4  54c1e3b9184cb5b6  abatement           forest region     A47   0.00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(data_path / \"train.csv\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7fcb9cc2-a38e-4821-bbe6-78afac6eb1ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>anchor</th>\n",
       "      <th>target</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "      <td>36473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>36473</td>\n",
       "      <td>733</td>\n",
       "      <td>29340</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>37d61fd2272659b1</td>\n",
       "      <td>component composite coating</td>\n",
       "      <td>composition</td>\n",
       "      <td>H01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>152</td>\n",
       "      <td>24</td>\n",
       "      <td>2186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                       anchor       target context\n",
       "count              36473                        36473        36473   36473\n",
       "unique             36473                          733        29340     106\n",
       "top     37d61fd2272659b1  component composite coating  composition     H01\n",
       "freq                   1                          152           24    2186"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe(include=\"object\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484d94e-7a58-4559-8a15-cc52ea3ae767",
   "metadata": {},
   "source": [
    "From having a quick look at a data sample and a description of the dataframe itself, there simply aren't that many unique contexts or anchors, and the anchor 'component composite coating' turns up 152 times which seems like a lot our of the 733 total.\n",
    "\n",
    "Lets now build our first feature which will be the 'input' feature. We'll structure this feature as we noted earlier in the TEXT1: ...; TEXT2:...; format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3412b0ea-3dda-407b-982a-487880f5731e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    TEXT1: A47; TEXT2: abatement of pollution; ANC1: abatement\n",
       "1            TEXT1: A47; TEXT2: act of abating; ANC1: abatement\n",
       "2           TEXT1: A47; TEXT2: active catalyst; ANC1: abatement\n",
       "3       TEXT1: A47; TEXT2: eliminating process; ANC1: abatement\n",
       "4             TEXT1: A47; TEXT2: forest region; ANC1: abatement\n",
       "Name: input, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['input'] = \"TEXT1: \" + df.context + \"; TEXT2: \" + df.target + \"; ANC1: \" + df.anchor\n",
    "\n",
    "df.input.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070e1d34-4d45-424c-890b-4adf8fe07e31",
   "metadata": {},
   "source": [
    "Ok this looks great, I'm already a bit confused since the dataset has described the df.context as the CPC classification subject and the anchor and target being the first and second phrase respectively whereas Jeremy has put the context as the first text. Nonethless, I'm sure the naming of each of these features is sort of irrelevant as long as its consistent. If I called it 'text1' or 'feature1' or 'f1' it wouldn't matter to the model, its all matrices in the end.\n",
    "\n",
    "\n",
    "### Tokenisation and Numericalisation\n",
    "Lets now take on the first two new topics which are tokenisation and numericalisation, this is the act of transforming our words into tokens which represent unique examples of words and turning those tokens into numbers which we can then matrix multiply and then convert back after the fact. We're going to use huggingface's datasets library and use their \"Dataset\" class which overlaps a class name with fastai and pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "525c6823-8a25-4a6d-b491-85795af31227",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'anchor', 'target', 'context', 'score', 'input'],\n",
       "    num_rows: 36473\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "ds = Dataset.from_pandas(df)\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705a5bf-8802-46b0-b65b-641bb2f349f0",
   "metadata": {},
   "source": [
    "Ok not learning a huge amount from the output but it at least lets us know what our features are and how many rows we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8bc40f37-e957-4bbf-8902-2dd68fa952a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<hr/>\n",
       "<h3>Dataset</h3>\n",
       "<blockquote><pre><code>Dataset(arrow_table:datasets.table.Table, info:Optional[datasets.info.DatasetInfo]=None, split:Optional[datasets.splits.NamedSplit]=None, indices_table:Optional[datasets.table.Table]=None, fingerprint:Optional[str]=None)</code></pre></blockquote><p>A Dataset backed by an Arrow table.</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc(Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d533ab-4298-4289-8b4f-e35a82708d25",
   "metadata": {},
   "source": [
    "Doesn't tell me a whole lot about the class or purpose but lets move on for now.\n",
    "\n",
    "An important thing to note is that particular tokenisers are used for particular models and if you want to use a pre-trained model, you have to make sure you have the same tokeniser (and numericalisation process) otherwise you aren't representing the language the same way as the model was trained. Lets grab a model and get to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a5d6d9-5112-4a52-8187-240da271b9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"microsoft/deberta-v3-small\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58556dbc-7d6a-4a87-8895-12485fcd7695",
   "metadata": {},
   "source": [
    "Now we're pulling from the [huggingface database](https://huggingface.co/models) of models, many of which are trained for particular tasks. Feel free to have a browse and search for models you might be interested in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1645965d-e81f-402d-a18b-c9584e86a9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "C:\\Users\\Nick\\Anaconda3\\envs\\fastai\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:446: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokeniser = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "026faa88-ef0f-4c14-b43a-afe398985ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='microsoft/deberta-v3-small', vocab_size=128000, model_max_len=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '[CLS]', 'eos_token': '[SEP]', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca744ed3-06b7-4d6c-9727-0a2e3e6c606f",
   "metadata": {},
   "source": [
    "Ok some interesting output here, looks like there's a bunch of settings on tokenisers that represent special handling of things like spaces or sizing a sentence with padding and separators etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "661bff4b-0113-4f51-b6f3-43313f02e256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁Jeremy', '▁is', '▁the', '▁GOAT']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.tokenize(\"Jeremy is the GOAT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911af5b1-4f53-4858-b37d-1cb9cd401afd",
   "metadata": {},
   "source": [
    "Ok awesome there are my tokens, Jeremy also notes that uncommon words will be split up and the start of words is represented by an underscore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fff98cd4-1842-4141-8a22-643ee5ac6e06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁I',\n",
       " '▁suffered',\n",
       " '▁a',\n",
       " '▁flab',\n",
       " 'berg',\n",
       " 'as',\n",
       " 'ting',\n",
       " '▁con',\n",
       " 'ni',\n",
       " 'ption',\n",
       " '▁when',\n",
       " '▁I',\n",
       " '▁saw',\n",
       " '▁the',\n",
       " '▁spelling',\n",
       " '▁of',\n",
       " '▁or',\n",
       " 'ni',\n",
       " 'tho',\n",
       " 'rhynch',\n",
       " 'us',\n",
       " '▁an',\n",
       " 'at',\n",
       " 'inus',\n",
       " '.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokeniser.tokenize(\"I suffered a flabbergasting conniption when I saw the spelling of ornithorhynchus anatinus.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8439ef9-0a68-4965-9091-9783ec62fb1a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
