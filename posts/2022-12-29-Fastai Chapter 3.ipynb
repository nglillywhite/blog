{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6cee887a",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /ML/fastai/jupyter/2022/12/26/Fastai Chapter 3\n",
    "badges: true\n",
    "categories:\n",
    "- jupyter\n",
    "- fastai\n",
    "- ML\n",
    "date: '2022-12-29'\n",
    "description: ReLu, Matrix Multiplication, Tensors, Gradient Descent\n",
    "output-file: 2022-12-29-fastai chapter 3.html\n",
    "title: Fastai Chapter 3 | Neural Net Foundations\n",
    "author: Nicholas Lillywhite\n",
    "draft: true\n",
    "toc: true\n",
    "image: \"images/shark.jpg\"\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60018f19-1234-438b-ba12-777cd35ca633",
   "metadata": {},
   "source": [
    "# Chapter 3 | Neural Nets Foundations\n",
    "> If You are Feeling Intimidated Like Me, Lets Work Through This Together\n",
    "\n",
    "This chapter focuses on understanding the absolute bare bones fundamentals of how deep learning works. In particular the individual calculations that are being done at each and every 'artificial neuron' of any deep learning network that we build. I'm certainly not from a math background and am generally intimidated but intrigued by learning math like this. I think I have shaky foundational math understanding but I'm hoping working through this course in great detail will bolster these core concepts and I can build on them. Despite my enjoyment of math in school, I did only the general curriculum in my final two years because the math teachers in my school were able to induce a coma purely via the audio of their voice. I have no doubt they were well intentioned people but many were phoning in the days and I wasn't willing to spend half of my school day with them. I'm paying the price for it now having to re-learn these concepts but maybe its for the best, if they'd explained that $y=mx+b$, derivatives, and quadratics could detect cancers and make self driving cars I probably would have been a more passionate student. I feel very much a product of the \"[Mathematician's Lament](https://www.maa.org/external_archive/devlin/LockhartsLament.pdf)\" that is referenced in chapter 1 of the book. Its a lovely read and I took many lessons away from the write-up, not only in how I want to teach things going forward but also a strong emotional response to how something as incredible as math is ruined and tarnished because of how its taught. Think of where we could be, but nonetheless this blog is about chapter 3 of the fastai course, not math education.\n",
    "\n",
    "Nonetheless here I am, and in the spirit of the \"Sidebar: Tenacity and Deep Learning\" from the book, I'm hoping that writing this blog and working through the content is a explicit evidence of success for me being both tenacious, and re-learning my math roots.\n",
    "\n",
    "## Main Topics\n",
    "\n",
    "The main concepts I want to have a 'mechanistic' & intuitive feeling for after this chapter are:\n",
    "\n",
    " - ReLu\n",
    " - Matrix Multiplication\n",
    " - Tensors\n",
    " - Gradient Descent\n",
    " \n",
    "Hopefully after reading this blog you also feel comfortable with these important tools and feel as comfortable as I intend to be implementing and discussing these core concepts.\n",
    "\n",
    "As mentioned in the lecture, this chapter has different content in the book from the lecture and I'd like to work through both, I'm firstly going to follow along the lecture with Jeremy and re-write & create the functions and tools he builds, barring the excel work which I'd like to re-write in python here, I will then work through the book content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59603e6f-a0d3-492e-93d6-cccecd43c64d",
   "metadata": {},
   "source": [
    "## Lecture Content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c83957-84d2-4bee-8fcb-9f36170045c1",
   "metadata": {},
   "source": [
    "### Timm Module\n",
    "\n",
    "Jeremy first talks through improving his pet classifier from the previous lesson, in particular having a look at different architectures and using the 'timm' library for vision model architectures. Lets have a look at the timm module and whats available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3b9216d-5f42-433d-85a7-000cdb67dfa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(964,\n",
       " ['adv_inception_v3',\n",
       "  'bat_resnext26ts',\n",
       "  'beit_base_patch16_224',\n",
       "  'beit_base_patch16_224_in22k',\n",
       "  'beit_base_patch16_384',\n",
       "  'beit_large_patch16_224',\n",
       "  'beit_large_patch16_224_in22k',\n",
       "  'beit_large_patch16_384',\n",
       "  'beit_large_patch16_512',\n",
       "  'beitv2_base_patch16_224',\n",
       "  'beitv2_base_patch16_224_in22k',\n",
       "  'beitv2_large_patch16_224',\n",
       "  'beitv2_large_patch16_224_in22k',\n",
       "  'botnet26t_256',\n",
       "  'botnet50ts_256',\n",
       "  'cait_m36_384',\n",
       "  'cait_m48_448',\n",
       "  'cait_s24_224',\n",
       "  'cait_s24_384',\n",
       "  'cait_s36_384'])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "len(timm.list_models()), timm.list_models()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288369e7-1949-4d27-b2ac-ba12218f51a5",
   "metadata": {},
   "source": [
    "There are a lot of models, almost ~1000 which is kind of nuts, looks like its certainly beefed up by different sizes of what I think is the same architecture structure, lets get a model down and have a look at the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "835d3747-98c4-4cc2-96d4-5e4f9b109a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (drop_block): Identity()\n",
       "      (act1): ReLU(inplace=True)\n",
       "      (aa): Identity()\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (act2): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet18 = timm.models.resnet18()\n",
    "resnet18"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fd81dc-860b-4cd5-b632-ef471256a6ec",
   "metadata": {},
   "source": [
    "Ok so I've brought down a 'tiny' model (in the scheme of todays models which have millions of parameters) called resnet16 which I used on my shark classifier in chapter 2 and the resnet architecture is what Jeremy references in the lecture.\n",
    "\n",
    "It looks like there are many 'Sequential' layers with 'BasicBlocks' inside them which then have a bunch of individual 'submodules' if we copy the language from the model.get_submodule() API which we're about to use. Lets now have a look at a particular submodule. The get_submodule() method allows us to step down the 'tree' and 'branches' of the layers with a dot notation. We shall step all the way down to a leaf, take particular note of the branch names contained within the smooth brackets '()'. First we go via the \"Layer1\" layer, into the first BasicBlock which has the notation of '(O)', I'm guessing because the layer is an array of BasicBlocks, the first index being 0, then I'm going to pick the BatchNorm2d submodule which has the notation of '(bn1)' within the brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48ee929c-900c-49f3-bef6-78cc529ec284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "         1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True),\n",
       " Parameter containing:\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        requires_grad=True)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = resnet18.get_submodule(\"layer1.0.bn1\")\n",
    "list(layer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194d1d9f-2d3a-4965-929c-ff5c9bb5cc5c",
   "metadata": {},
   "source": [
    "OK so we've got a couple of tensors, one set to all ones and another set to all zeroes, lets have a look at the doc to see if there's any hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aed2fe06-c1b4-43f5-8b38-7cbc31acc206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[1;31mInit signature:\u001b[0m\n",
       "\u001b[0mBatchNorm2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mnum_features\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0meps\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-05\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mmomentum\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0maffine\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
       "\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
       "\u001b[1;31mDocstring:\u001b[0m     \n",
       "Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs\n",
       "with additional channel dimension) as described in the paper\n",
       "`Batch Normalization: Accelerating Deep Network Training by Reducing\n",
       "Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .\n",
       "\n",
       ".. math::\n",
       "\n",
       "    y = \\frac{x - \\mathrm{E}[x]}{ \\sqrt{\\mathrm{Var}[x] + \\epsilon}} * \\gamma + \\beta\n",
       "\n",
       "The mean and standard-deviation are calculated per-dimension over\n",
       "the mini-batches and :math:`\\gamma` and :math:`\\beta` are learnable parameter vectors\n",
       "of size `C` (where `C` is the input size). By default, the elements of :math:`\\gamma` are set\n",
       "to 1 and the elements of :math:`\\beta` are set to 0. The standard-deviation is calculated\n",
       "via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.\n",
       "\n",
       "Also by default, during training this layer keeps running estimates of its\n",
       "computed mean and variance, which are then used for normalization during\n",
       "evaluation. The running estimates are kept with a default :attr:`momentum`\n",
       "of 0.1.\n",
       "\n",
       "If :attr:`track_running_stats` is set to ``False``, this layer then does not\n",
       "keep running estimates, and batch statistics are instead used during\n",
       "evaluation time as well.\n",
       "\n",
       ".. note::\n",
       "    This :attr:`momentum` argument is different from one used in optimizer\n",
       "    classes and the conventional notion of momentum. Mathematically, the\n",
       "    update rule for running statistics here is\n",
       "    :math:`\\hat{x}_\\text{new} = (1 - \\text{momentum}) \\times \\hat{x} + \\text{momentum} \\times x_t`,\n",
       "    where :math:`\\hat{x}` is the estimated statistic and :math:`x_t` is the\n",
       "    new observed value.\n",
       "\n",
       "Because the Batch Normalization is done over the `C` dimension, computing statistics\n",
       "on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.\n",
       "\n",
       "Args:\n",
       "    num_features: :math:`C` from an expected input of size\n",
       "        :math:`(N, C, H, W)`\n",
       "    eps: a value added to the denominator for numerical stability.\n",
       "        Default: 1e-5\n",
       "    momentum: the value used for the running_mean and running_var\n",
       "        computation. Can be set to ``None`` for cumulative moving average\n",
       "        (i.e. simple average). Default: 0.1\n",
       "    affine: a boolean value that when set to ``True``, this module has\n",
       "        learnable affine parameters. Default: ``True``\n",
       "    track_running_stats: a boolean value that when set to ``True``, this\n",
       "        module tracks the running mean and variance, and when set to ``False``,\n",
       "        this module does not track such statistics, and initializes statistics\n",
       "        buffers :attr:`running_mean` and :attr:`running_var` as ``None``.\n",
       "        When these buffers are ``None``, this module always uses batch statistics.\n",
       "        in both training and eval modes. Default: ``True``\n",
       "\n",
       "Shape:\n",
       "    - Input: :math:`(N, C, H, W)`\n",
       "    - Output: :math:`(N, C, H, W)` (same shape as input)\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # With Learnable Parameters\n",
       "    >>> m = nn.BatchNorm2d(100)\n",
       "    >>> # Without Learnable Parameters\n",
       "    >>> m = nn.BatchNorm2d(100, affine=False)\n",
       "    >>> input = torch.randn(20, 100, 35, 45)\n",
       "    >>> output = m(input)\n",
       "\u001b[1;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[1;31mFile:\u001b[0m           c:\\users\\nick\\anaconda3\\envs\\fastai\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\n",
       "\u001b[1;31mType:\u001b[0m           type\n",
       "\u001b[1;31mSubclasses:\u001b[0m     BatchNormAct2d, SplitBatchNorm2d\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from torch.nn import BatchNorm2d\n",
    "\n",
    "BatchNorm2d?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99a37f-01d6-4c2e-aa7c-d9c232e33670",
   "metadata": {},
   "source": [
    "Ok deadset I'm not sure what a lot, if not all of this means, but there's a nice link to the paper that proposed this submodule. Maybe I'll revisit this later or what I'm guessing is that we will discuss batch normalisation as part of the course. Nonetheless, credit due to the pytorch team for awesome docs and references. I'm certainly feeling comfortable picking apart a model and submodules to then research or understand the peices. And to Jeremy's point, it looks like each module is just tensors which I'm assuming get matrix multiplied.\n",
    "\n",
    "Lets have a look at another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85871cda-3f6d-4bf6-86c6-0fff6f7c0b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parameter containing:\n",
       " tensor([[[[-1.2502e-01,  2.1611e-02,  6.2673e-02],\n",
       "           [ 4.5127e-02,  3.0546e-02, -4.1605e-04],\n",
       "           [ 2.3156e-02, -3.6311e-02,  3.3031e-03]],\n",
       " \n",
       "          [[ 1.4144e-02, -6.9305e-02, -4.6877e-02],\n",
       "           [ 3.5611e-02,  7.7496e-02, -3.8019e-02],\n",
       "           [ 2.2473e-02,  1.5468e-02, -8.8914e-02]],\n",
       " \n",
       "          [[ 1.1887e-01,  3.5537e-02, -3.4866e-02],\n",
       "           [-4.2336e-02, -9.2128e-02, -3.5970e-02],\n",
       "           [ 2.1227e-02,  6.4185e-02,  1.1702e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 4.3194e-02,  4.8263e-02, -4.6128e-02],\n",
       "           [-2.4847e-02,  1.1796e-02, -4.1562e-02],\n",
       "           [-1.4793e-03, -7.6328e-02, -2.8772e-03]],\n",
       " \n",
       "          [[-3.0051e-02, -1.6366e-02, -1.0406e-02],\n",
       "           [ 4.4385e-02, -6.6810e-02, -1.4850e-02],\n",
       "           [-4.8456e-02,  1.0097e-02, -2.8386e-02]],\n",
       " \n",
       "          [[ 7.7507e-03, -4.8761e-02,  1.6333e-02],\n",
       "           [-2.8063e-02, -7.2671e-02,  4.1981e-02],\n",
       "           [-5.1363e-03, -8.3430e-03,  1.6157e-02]]],\n",
       " \n",
       " \n",
       "         [[[-4.1581e-02, -8.6135e-02, -8.8504e-03],\n",
       "           [-5.1267e-02, -1.3126e-02, -5.1560e-02],\n",
       "           [ 4.2636e-02, -8.3282e-02,  2.4826e-02]],\n",
       " \n",
       "          [[ 3.0648e-02,  1.4557e-02,  5.5446e-02],\n",
       "           [ 1.6359e-02,  1.5561e-04, -2.9289e-02],\n",
       "           [ 1.1688e-02,  1.4299e-02, -1.1891e-02]],\n",
       " \n",
       "          [[ 7.0961e-02, -7.9051e-02, -9.2660e-02],\n",
       "           [ 5.3902e-02, -3.4843e-02, -6.2368e-02],\n",
       "           [-6.9385e-02, -3.7986e-02, -1.0079e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 5.6354e-02,  1.2459e-02, -5.7815e-02],\n",
       "           [-1.8793e-02,  1.9939e-02, -4.9465e-02],\n",
       "           [ 4.8720e-02, -8.0751e-02,  5.0426e-02]],\n",
       " \n",
       "          [[-9.9178e-02, -2.6330e-02,  6.4110e-02],\n",
       "           [ 1.5294e-01, -1.1381e-02, -6.8076e-02],\n",
       "           [-6.1193e-03,  1.8105e-04,  1.1624e-01]],\n",
       " \n",
       "          [[-1.6387e-02,  4.3791e-02, -4.4517e-02],\n",
       "           [ 2.0552e-02,  8.2836e-02, -1.0525e-01],\n",
       "           [ 7.2949e-03,  5.7074e-02, -6.2815e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 1.1438e-02,  4.6379e-02,  6.4386e-02],\n",
       "           [-1.6729e-03, -3.1183e-02,  1.7626e-03],\n",
       "           [ 6.4510e-02, -6.5706e-02, -2.3200e-02]],\n",
       " \n",
       "          [[ 6.9316e-03,  4.3203e-02,  2.4106e-02],\n",
       "           [-2.7356e-02, -2.1419e-02,  3.6932e-02],\n",
       "           [ 4.7467e-02,  3.0317e-03, -1.1237e-01]],\n",
       " \n",
       "          [[ 2.2952e-02,  1.5698e-02, -5.0379e-02],\n",
       "           [ 3.0202e-02, -5.5555e-02,  3.2085e-02],\n",
       "           [ 2.6488e-03, -5.6769e-02, -4.4469e-03]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 6.8578e-03, -9.8037e-03, -1.6990e-02],\n",
       "           [-7.9334e-02,  4.8124e-02,  2.4739e-02],\n",
       "           [ 1.1484e-01,  9.3034e-02,  1.2796e-02]],\n",
       " \n",
       "          [[-7.2724e-02,  2.7770e-02,  5.7036e-02],\n",
       "           [ 2.3826e-02,  8.7389e-02,  1.1097e-01],\n",
       "           [-3.4996e-02,  1.0612e-01,  1.2499e-01]],\n",
       " \n",
       "          [[-3.8849e-02,  2.4923e-02,  5.4723e-02],\n",
       "           [ 1.6369e-02,  2.1655e-02,  4.9131e-02],\n",
       "           [ 3.0290e-02, -1.6581e-02,  2.4901e-02]]],\n",
       " \n",
       " \n",
       "         ...,\n",
       " \n",
       " \n",
       "         [[[ 1.2705e-03, -3.0859e-02,  2.9601e-02],\n",
       "           [ 1.1177e-02,  7.7071e-02,  3.4052e-02],\n",
       "           [ 3.0717e-02,  1.6784e-01,  8.2709e-02]],\n",
       " \n",
       "          [[-2.4277e-02, -1.0960e-01,  7.1699e-02],\n",
       "           [-6.5604e-02,  4.4265e-02,  2.6033e-02],\n",
       "           [-3.0962e-02,  1.0067e-01,  6.6478e-02]],\n",
       " \n",
       "          [[ 7.7911e-03,  3.5157e-02,  1.6410e-02],\n",
       "           [ 3.9729e-02, -7.4866e-02,  4.6392e-03],\n",
       "           [ 1.8479e-02,  2.6517e-02, -3.6581e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[ 3.2488e-02,  2.6283e-02, -8.5648e-02],\n",
       "           [-4.6623e-02, -6.0345e-02, -5.3536e-02],\n",
       "           [-5.7190e-02, -1.9372e-03, -1.6878e-02]],\n",
       " \n",
       "          [[ 3.6678e-02,  9.0045e-03, -1.1511e-01],\n",
       "           [-6.6708e-03, -1.3391e-02, -9.2928e-02],\n",
       "           [ 8.8528e-02,  7.1037e-02,  1.5657e-02]],\n",
       " \n",
       "          [[ 9.4261e-03,  2.4607e-02, -2.1281e-02],\n",
       "           [-1.3614e-02, -9.0740e-02,  6.3674e-02],\n",
       "           [-3.7089e-02, -8.7983e-02,  8.9460e-03]]],\n",
       " \n",
       " \n",
       "         [[[ 5.5799e-02,  3.0267e-02, -2.3158e-02],\n",
       "           [ 7.1850e-02, -6.4913e-03,  3.5413e-02],\n",
       "           [ 2.7945e-02, -6.2676e-02, -6.5168e-02]],\n",
       " \n",
       "          [[-4.3612e-04, -1.0108e-01, -6.3748e-02],\n",
       "           [ 4.1953e-02, -4.8596e-02, -5.7594e-02],\n",
       "           [-3.1175e-02,  3.3190e-02,  1.5191e-02]],\n",
       " \n",
       "          [[ 4.7058e-02,  2.4500e-02, -3.9941e-02],\n",
       "           [ 4.3219e-02,  9.3919e-02, -1.8494e-02],\n",
       "           [ 1.7171e-02,  6.7686e-02,  4.2446e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-7.9894e-02, -1.2839e-02,  3.8433e-02],\n",
       "           [ 4.4090e-02,  2.1679e-03, -3.2518e-02],\n",
       "           [-1.3204e-01, -2.1713e-02, -9.8557e-02]],\n",
       " \n",
       "          [[ 6.5861e-02, -2.0002e-02,  4.5157e-02],\n",
       "           [-6.0203e-02, -3.8577e-02,  3.9418e-02],\n",
       "           [-3.5612e-02, -7.5511e-02, -1.5652e-02]],\n",
       " \n",
       "          [[ 1.5506e-02, -1.6654e-02, -3.0249e-02],\n",
       "           [ 4.1769e-02,  4.0004e-02, -1.8587e-02],\n",
       "           [-3.4219e-02,  7.2060e-02,  1.8002e-02]]],\n",
       " \n",
       " \n",
       "         [[[ 3.7934e-02,  6.7550e-02, -4.0581e-02],\n",
       "           [-2.3039e-01, -7.8151e-02, -8.1118e-03],\n",
       "           [ 5.3113e-02, -5.7478e-02, -6.4390e-03]],\n",
       " \n",
       "          [[ 5.1016e-02, -5.2449e-02, -1.2150e-01],\n",
       "           [ 1.5673e-02, -1.6938e-01,  8.3085e-03],\n",
       "           [ 1.1933e-01,  5.4104e-03, -4.4078e-02]],\n",
       " \n",
       "          [[ 7.4732e-02, -2.1445e-03,  8.4983e-02],\n",
       "           [ 2.4126e-02,  5.2747e-02, -9.2056e-02],\n",
       "           [ 3.2234e-02,  6.0205e-02, -2.0604e-02]],\n",
       " \n",
       "          ...,\n",
       " \n",
       "          [[-2.1415e-02,  9.7487e-02,  1.6328e-02],\n",
       "           [-5.3877e-03, -8.5096e-02, -5.4508e-02],\n",
       "           [ 5.6044e-02,  8.1858e-02,  2.6316e-03]],\n",
       " \n",
       "          [[-2.5481e-02, -2.6980e-02,  2.5018e-02],\n",
       "           [ 1.2155e-02,  7.7784e-02, -2.7443e-02],\n",
       "           [-6.1501e-02, -3.9937e-02, -2.2548e-02]],\n",
       " \n",
       "          [[ 1.2036e-02,  1.0327e-02, -1.7388e-02],\n",
       "           [ 4.2615e-02,  1.9255e-02, -2.3732e-02],\n",
       "           [-5.6301e-02, -9.9942e-02, -6.2528e-02]]]], requires_grad=True)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = resnet18.get_submodule(\"layer1.0.conv2\")\n",
    "list(layer.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b2cde-105a-49b5-9ed2-f4a58f4d7ecb",
   "metadata": {},
   "source": [
    "Ok lets stop that, this thing is big. But nonetheless its interesting to see the different shapes and values, the batch norm module had zeroes and ones, this one seems to have all sorts of values and the shape is very different.\n",
    "\n",
    "As Jeremy mentions, apparently these numbers can figure out if a dog is a basset hound or not, or in our previous example, a great white shark or a hammerhead. However this isn't clear at this time. Again as Jeremy mentions, machine learning is the act of fitting a function to data, lets investigate this further.\n",
    "\n",
    "### How Do We Fit a Function to Data\n",
    "\n",
    "Lets first build a general quadratic equation and plot it. I don't actually have an intuitive feeling for what makes this a 'quadratic' but again in the spirit of 80% do and 20% study, I'm going to soldier on to see the 'ball game' played out and circle back later to solidfy my theory as part of the 20% reading principle outlined in Radek's Metalearning book which I love. Note to Radek, I'm trusting you that this is a good plan, its working so far but as a product of school doing the opposite, I feel very conflicted moving on without actually 'knowing'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1d47de-4628-462b-bbed-c9dcbfd54c8b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_function' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(x): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m3\u001b[39m\u001b[38;5;241m*\u001b[39mx\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mplot_function\u001b[49m(f, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m$3x^2 + 2x + 1$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plot_function' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def f(x): return 3*x**2 + 2*x + 1\n",
    "\n",
    "plot_function(f, title=\"$3x^2 + 2x + 1$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca64f01-50d4-4e99-8655-bd51df571d2d",
   "metadata": {},
   "source": [
    "This f(x) function is nice to plot that particular function but it'd be nice to be able to play with the parameters, so lets define a quad() function where we can pass in what we like.\n",
    "\n",
    "Also functionally these two definitions of a function are the same that I've written below, its just a nice python syntax to be able to write it on one line but its not very common in the general python universe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f12167b-a880-40ef-9d6d-b476247f66f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quad(a,b,c,x): return a*x**2 + b*x + c\n",
    "\n",
    "def quad(a,b,c,x):\n",
    "    return a*x**2 + b*x + c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e8083-e4e0-4df9-8db8-5162771c8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "quad(3,2,1,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10378769-8587-48cd-af48-635bb0831ac4",
   "metadata": {},
   "source": [
    "Lets introduce as Jeremy does partial functions, he describes it as something along the lines of 'fixing' part of a function. I've thought of it as making a modified function from another function but his description is simpler."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb09cc2-b144-4fdd-a877-b1f3a64b8158",
   "metadata": {},
   "source": [
    "#### Partial Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4fa2f2-8576-4faa-b96f-1565901deb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def mk_quad(a,b,c): return partial(quad,a,b,c)\n",
    "f = mk_quad(3,2,1)\n",
    "f(1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4586dd-895b-4179-813e-33e54f676775",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088eef81-ee4c-400a-ad69-296fa299c36b",
   "metadata": {},
   "source": [
    "The [python docs themselves are quite useful](https://docs.python.org/3/library/functools.html#functools.partial) at describing partials. For example, \"Return a new partial object which when called will behave like *func* called with the positional and keyword arguments.\"\n",
    "\n",
    "\"The partial() is used for *partial function application* which \"freezes\" some portion of a function's arguements and keywords, resulting in a new object with a simplified signature.\"\n",
    "\n",
    "Looks like Jeremy is more accurate, closer to the original python docs & its probably a better analogy of partial objects. My understanding is improved and I'll stop saying function from another function and start espousing something similar to the docs & Jeremy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ff2df-88b4-4b9a-bb84-c4d8aed67ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_function(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27df66-1a87-447a-a7f7-31975d19a280",
   "metadata": {},
   "source": [
    "#### Adding Noise to Our Perfect Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2999cf96-7a19-4a83-81e0-06f0946fc5cb",
   "metadata": {},
   "source": [
    "Lets now make some 'real' looking data,  we can add some noise to this function to more closely represent what data we're more likely to spot out in the hypothetical real world where the generator function of our data is perfect like this but we also live in the same world with innacurate measuring devices.\n",
    "\n",
    "#### Nassim Taleb is an Awesome Writer\n",
    "\n",
    "Side note, the book \"Fooled by Randomness\" and \"Black Swan\" by Nassim Taleb are genuinely inspiring works that made me think and behave about and in the world differently. In particular Nassim introduces the concept of these 'invisible' generators that create the randomness in our world, the main problem is that we only ever observe a sample from these generators. Despite 'long' time frames relative to our lives, ie having data over 20 years, that simply might be an insufficient sample from the 'generator' to make any worthwhile inference of what the actual likelihood of your observations actually are. Irrelevant to the python we're writing right now but when imaginging this hypothetical world where we observe this perfect function but only see a noisy version, I thought I'd share some of my favourite books."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957bed43-7456-4ab9-9a74-1acf97a4694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import normal, seed, uniform\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def noise(x, scale): return normal(scale=scale, size=x.shape)\n",
    "def add_noise(x, mult, add): return x * (1+noise(x, mult)) + noise(x,add)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c19947-54a0-4116-b027-8e7eeabf9665",
   "metadata": {},
   "source": [
    "Lets investigate each of the variables that Jeremy instantiates in the next few lines. I want to understand what each method is doing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879dbf3d-8ad6-478b-8065-c7794fe96fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc(normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f38c25f-2a90-4a6d-8abf-287b57b9e338",
   "metadata": {},
   "source": [
    "Ok so the normal function will draw random samples from a normal distribution, we have a scale and size variable which set the standard deviation and number of outputs we'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6737fe6-85bc-49a7-8016-f2473bd00e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see 10 samples which are taken from a normal distribution with a standard deviation of 0.3\n",
    "\n",
    "normal(scale=.3,size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c841d1b5-bb2f-4588-a314-05a45fa56f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.linspace?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad98fbce-b654-4b36-96aa-86b106bbd012",
   "metadata": {},
   "source": [
    "torch.linspace looks like a really nice way to build a tensor that I think is 'linearly' spaced out based on the start,stop, and steps variables you provide. So below we start from -2, go all the way to 2, and add 20 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1a5d8-80c5-4368-81f1-45a268826ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = torch.linspace(-2,2,steps=20)\n",
    "test, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7b6eb-d456-4499-8071-cdcb405fe19d",
   "metadata": {},
   "source": [
    "Jeremy also runs a '[:,None]' indexation on this linspace which seems like a cool trick to do something but I'm not quite sure what. It look like he wants all of the columns, hence the ';' semi-colon which gives you all but I'm not sure what the None command does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9416c41-5b29-4f84-a364-ab2733f32cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test[:,None], test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7dcde7b-ab72-4184-a777-594b9704dfdf",
   "metadata": {},
   "source": [
    "Ok so it looks like it transposes the tensor from being a single row with many columns to being one column with many rows. I think my language of 'rows' and 'columns' is incorrect, this is simply a data table / dataframe way of thinking and tensors are fundamentally different so I need to figure out better language but I'm hoping we're at a simple enough state where this makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82750fc-2ebe-4f3e-9fbd-356f52141623",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_noise(f(test),.3,1.5),add_noise(f(test)[:,None],.3,1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5647da02-90ec-4a06-a679-a78cbd65542e",
   "metadata": {},
   "source": [
    "Ok so it looks like the same kind of data but transposed as we saw before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1275794-c1d9-40b8-bb06-98b817310bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2, 2, steps=20)[:,None]\n",
    "y = add_noise(f(x), 0.3, 1.5)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67343d4c-1a6a-4d84-a85f-4d904f638a00",
   "metadata": {},
   "source": [
    "Lets try it without the transposing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a691cfc-550d-4a80-bb76-ef51c294aeba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.linspace(-2,2,steps=20)\n",
    "y = add_noise(f(x),0.3,1.5)\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc11e541-7aca-448c-8a22-e7a043186836",
   "metadata": {},
   "source": [
    "Ok looks the same, Jeremy not sure why we did this transposing of a 1d tensor but its certainly a neat trick. Lets move on and start playing with some parameters and Ipython interactivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba29f217-8e25-4cf2-a731-8dbdfb33ac87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "@interact(a=1.5, b=1.5, c=1.5)\n",
    "def plot_quad(a,b,c):\n",
    "    plot_function(mk_quad(a,b,c))\n",
    "    plt.scatter(x,y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9f5112-4221-469c-a269-3f750d321503",
   "metadata": {},
   "source": [
    "Now if you're reading on quarto, I recognise that you won't be able to play with the plot I've written above so I've re-written a plot function command with Altair so that you can play around with it on the blog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4932ca1-6c65-4903-acd9-4ff16c198452",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c = 1.5, 1.5, 1\n",
    "\n",
    "x = torch.linspace(-2,2,steps=20)\n",
    "y = add_noise(f(x),0.3,1.5)\n",
    "data = pd.DataFrame({\"x\":x.numpy(), \"y\":y.numpy()})\n",
    "scatter = alt.Chart(data).mark_point().encode(\n",
    "    x='x:Q',\n",
    "    y='y:Q'\n",
    ")\n",
    "\n",
    "f = mk_quad(a,b,c)\n",
    "\n",
    "selector_a =  alt.selection_single(name=\"selector_a\", \n",
    "                                fields=['a'],  \n",
    "                                bind=alt.binding_range(min=0, max=3, step=0.1, name='A'),\n",
    "                                init={'a': a,}) \n",
    "selector_b = alt.selection_single(name=\"selector_b\",\n",
    "                                fields=['b'], \n",
    "                                bind=alt.binding_range(min=0, max=3, step=0.1, name='B'),\n",
    "                                init={'b': b})\n",
    "selector_c = alt.selection_single(name=\"selector_c\",\n",
    "                                fields=['c'], \n",
    "                                bind=alt.binding_range(min=0, max=3, step=0.1, name='C'),\n",
    "                                init={'c': c})\n",
    "\n",
    "\n",
    "line = alt.Chart(pd.DataFrame({\"x\":x.numpy(),\"y\":1,\"c\":c})).mark_line(color=\"red\").encode(\n",
    "    x='x',\n",
    "    y='y',\n",
    ").transform_calculate(\n",
    "    y=\"((selector_a.a*pow(datum.x,2) + selector_b.b*datum.x)) + datum.c\").properties(title='a*x^2 + b*x + c').add_selection(selector_b).add_selection(selector_a)\n",
    "\n",
    "(line + scatter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec07e6a-b5e0-4bbc-98a9-c34d014eb3c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4d8886-2cec-4d07-94f2-c4f76d64a3c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d104c76-bfa7-44a1-84c4-7ed897854575",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff6f2e1-79ee-44c6-ba9c-439670c9deb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "963cd2ce-40ff-40a4-9856-8cbf22fac394",
   "metadata": {},
   "source": [
    "## Book Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa72c10-00fc-4320-be6d-4e1ac67d2797",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
